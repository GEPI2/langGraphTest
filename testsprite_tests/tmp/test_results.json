[
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "249e952a-e9c2-48c9-bf3a-b4b754574747",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC001-test_graph_editor_node_drag_and_drop",
    "description": "Verify that nodes can be dragged and dropped successfully in the Graph Editor and that connections between nodes can be created and managed in real-time.",
    "code": "import requests\n\nBASE_URL = \"http://localhost:8001\"\nHEADERS = {\"Content-Type\": \"application/json\"}\nTIMEOUT = 30\n\ndef test_graph_editor_node_drag_and_drop():\n    # Step 1: Create two nodes (simulate drag and drop create)\n    node_payload_1 = {\n        \"type\": \"LLMNode\",\n        \"data\": {\"label\": \"Node 1\", \"model\": \"gpt-4\"},\n        \"position\": {\"x\": 100, \"y\": 200}\n    }\n    node_payload_2 = {\n        \"type\": \"RAGNode\",\n        \"data\": {\"label\": \"Node 2\", \"source\": \"knowledge_base\"},\n        \"position\": {\"x\": 300, \"y\": 200}\n    }\n    created_node_1 = None\n    created_node_2 = None\n\n    # Step 2: Function to delete node to cleanup\n    def delete_node(node_id):\n        try:\n            resp = requests.delete(f\"{BASE_URL}/graph/node/{node_id}\", timeout=TIMEOUT)\n            # May be 204 No Content or 200 OK\n            assert resp.status_code in (200, 204)\n        except Exception:\n            pass  # best effort cleanup\n\n    try:\n        # Create first node\n        resp1 = requests.post(f\"{BASE_URL}/graph/node\", json=node_payload_1, headers=HEADERS, timeout=TIMEOUT)\n        assert resp1.status_code == 201, f\"Failed to create node 1: {resp1.text}\"\n        created_node_1 = resp1.json().get(\"id\")\n        assert created_node_1 is not None, \"Node 1 creation response missing id\"\n\n        # Create second node\n        resp2 = requests.post(f\"{BASE_URL}/graph/node\", json=node_payload_2, headers=HEADERS, timeout=TIMEOUT)\n        assert resp2.status_code == 201, f\"Failed to create node 2: {resp2.text}\"\n        created_node_2 = resp2.json().get(\"id\")\n        assert created_node_2 is not None, \"Node 2 creation response missing id\"\n\n        # Step 3: Create connection between nodes\n        connection_payload = {\n            \"source\": created_node_1,\n            \"target\": created_node_2,\n            \"type\": \"default\"  # Assuming default connection type\n        }\n        resp_conn = requests.post(f\"{BASE_URL}/graph/connection\", json=connection_payload, headers=HEADERS, timeout=TIMEOUT)\n        assert resp_conn.status_code == 201, f\"Failed to create connection: {resp_conn.text}\"\n        connection_id = resp_conn.json().get(\"id\")\n        assert connection_id is not None, \"Connection creation response missing id\"\n\n        # Step 4: Retrieve connections to verify real-time management\n        resp_list = requests.get(f\"{BASE_URL}/graph/connection\", timeout=TIMEOUT)\n        assert resp_list.status_code == 200, f\"Failed to list connections: {resp_list.text}\"\n        connections = resp_list.json()\n        assert any(c.get(\"id\") == connection_id for c in connections), \"Created connection not found in list\"\n\n        # Step 5: Delete the connection to test connection management (removal)\n        resp_del_conn = requests.delete(f\"{BASE_URL}/graph/connection/{connection_id}\", timeout=TIMEOUT)\n        assert resp_del_conn.status_code in (200, 204), f\"Failed to delete connection: {resp_del_conn.text}\"\n\n        # Verify connection deletion\n        resp_list_after_del = requests.get(f\"{BASE_URL}/graph/connection\", timeout=TIMEOUT)\n        assert resp_list_after_del.status_code == 200, f\"Failed to list connections after delete: {resp_list_after_del.text}\"\n        connections_after_del = resp_list_after_del.json()\n        assert all(c.get(\"id\") != connection_id for c in connections_after_del), \"Connection was not deleted properly\"\n\n    finally:\n        if created_node_1:\n            delete_node(created_node_1)\n        if created_node_2:\n            delete_node(created_node_2)\n\ntest_graph_editor_node_drag_and_drop()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 77, in <module>\n  File \"<string>\", line 34, in test_graph_editor_node_drag_and_drop\nAssertionError: Failed to create node 1: {\"detail\":\"Not Found\"}\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.699Z",
    "modified": "2025-12-05T11:05:35.969Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "d2d29da4-59be-490c-bb86-904e9d70cbf0",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC002-test_graph_execution_trigger",
    "description": "Test that executing a graph triggers the appropriate backend API calls and returns successful execution status.",
    "code": "import requests\n\nBASE_URL = \"http://localhost:8001\"\nTIMEOUT = 30\nHEADERS = {\"Content-Type\": \"application/json\"}\n\ndef test_graph_execution_trigger():\n    graph_payload = {\n        \"id\": \"test-graph-1\",\n        \"name\": \"Test Graph\",\n        \"nodes\": [\n            {\"id\": \"START\", \"type\": \"START\", \"data\": {}},\n            {\"id\": \"node1\", \"type\": \"LLMNode\", \"data\": {\"prompt\": \"Hello\"}},\n            {\"id\": \"node2\", \"type\": \"CodeNode\", \"data\": {\"code\": \"print('test')\"}}\n        ],\n        \"edges\": [\n            {\"source\": \"START\", \"target\": \"node1\"},\n            {\"source\": \"node1\", \"target\": \"node2\"}\n        ]\n    }\n\n    created_graph_id = None\n    try:\n        # Create a new graph configuration\n        create_resp = requests.post(\n            f\"{BASE_URL}/graphs\",\n            json=graph_payload,\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert create_resp.status_code == 201, f\"Graph creation failed: {create_resp.text}\"\n        created_graph = create_resp.json()\n        created_graph_id = created_graph.get(\"id\")\n        assert created_graph_id is not None, \"Created graph ID is None\"\n\n        # Execute the created graph\n        execute_resp = requests.post(\n            f\"{BASE_URL}/graphs/{created_graph_id}/execute\",\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert execute_resp.status_code == 200, f\"Graph execution failed: {execute_resp.text}\"\n        execution_result = execute_resp.json()\n        # Validate expected fields in execution result\n        assert \"status\" in execution_result, \"Missing 'status' in execution response\"\n        assert execution_result[\"status\"] == \"success\", f\"Unexpected execution status: {execution_result['status']}\"\n        assert \"execution_id\" in execution_result, \"Missing 'execution_id' in execution response\"\n\n    finally:\n        if created_graph_id is not None:\n            # Clean up by deleting the created graph\n            try:\n                del_resp = requests.delete(\n                    f\"{BASE_URL}/graphs/{created_graph_id}\",\n                    headers=HEADERS,\n                    timeout=TIMEOUT,\n                )\n                assert del_resp.status_code in (200, 204), f\"Graph deletion failed: {del_resp.text}\"\n            except Exception:\n                pass\n\ntest_graph_execution_trigger()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 62, in <module>\n  File \"<string>\", line 31, in test_graph_execution_trigger\nAssertionError: Graph creation failed: {\"detail\":[{\"type\":\"literal_error\",\"loc\":[\"body\",\"nodes\",0,\"type\"],\"msg\":\"Input should be 'LLMNode', 'CodeNode', 'RAGNode', 'HumanNode', 'StartNode' or 'EndNode'\",\"input\":\"START\",\"ctx\":{\"expected\":\"'LLMNode', 'CodeNode', 'RAGNode', 'HumanNode', 'StartNode' or 'EndNode'\"}}]}\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.705Z",
    "modified": "2025-12-05T11:05:31.955Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "25b784be-9a14-4740-b1ea-79f12a7da55f",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC003-test_rag_dashboard_document_upload",
    "description": "Validate that documents can be uploaded to the RAG knowledge base through the RAG Dashboard and that the upload process returns appropriate success or error responses.",
    "code": "import requests\nimport os\n\nBASE_URL = \"http://localhost:8001\"\nTIMEOUT = 30\n\ndef test_rag_dashboard_document_upload():\n    upload_url = f\"{BASE_URL}/rag/upload\"\n    \n    file_content = b\"Sample content for RAG knowledge base document upload test.\"\n    files = {\n        'file': ('test_document.txt', file_content, 'text/plain')\n    }\n    \n    try:\n        response = requests.post(upload_url, files=files, timeout=TIMEOUT)\n    except requests.RequestException as e:\n        assert False, f\"Request to upload document failed: {e}\"\n    \n    assert response.status_code in (200, 201), f\"Unexpected status code: {response.status_code}, response: {response.text}\"\n    \n    try:\n        resp_json = response.json()\n    except ValueError:\n        assert False, f\"Response is not valid JSON: {response.text}\"\n    \n    assert ('filename' in resp_json and 'status' in resp_json), \\\n        f\"Response JSON missing expected keys: {resp_json}\"\n    \n    if response.status_code not in (200, 201):\n        assert 'error' in resp_json or 'detail' in resp_json, f\"Expected error details in response: {resp_json}\"\n\ntest_rag_dashboard_document_upload()\n",
    "testStatus": "PASSED",
    "testError": "",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.710Z",
    "modified": "2025-12-05T11:05:30.454Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "3c879f85-d32e-45af-b17b-deaa7b568003",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC004-test_rag_dashboard_retrieval_testing",
    "description": "Ensure that the RAG Dashboard allows testing of information retrieval from uploaded documents and returns accurate retrieval results.",
    "code": "import requests\nfrom io import BytesIO\n\nBASE_URL = \"http://localhost:8001\"\nTIMEOUT = 30\n\ndef test_rag_dashboard_retrieval_testing():\n    # Step 1: Upload a sample document to create a knowledge base entry\n    upload_url = f\"{BASE_URL}/rag/upload\"\n    retrieval_url = f\"{BASE_URL}/rag/retrieve\"\n\n    # Using multipart/form-data upload with a file field named 'file'\n    file_content = b\"LangGraph Builder allows users to create complex AI agent workflows using a visual editor.\"\n    files = {\"file\": (\"sample.txt\", BytesIO(file_content), \"text/plain\")}\n\n    # Upload the document\n    upload_resp = requests.post(upload_url, files=files, timeout=TIMEOUT)\n    assert upload_resp.status_code == 200, f\"Document upload failed: {upload_resp.status_code} {upload_resp.text}\"\n    upload_data = upload_resp.json()\n    assert upload_data.get(\"success\") is True or upload_data.get(\"status\") == \"ok\", \"Upload response missing success status\"\n\n    # Step 2: Test retrieval of information related to the uploaded document\n    query_payload = {\n        \"query\": \"What does the LangGraph Builder allow users to do?\"\n    }\n\n    retrieval_resp = requests.post(retrieval_url, json=query_payload, timeout=TIMEOUT)\n    assert retrieval_resp.status_code == 200, f\"Retrieval request failed: {retrieval_resp.status_code} {retrieval_resp.text}\"\n    retrieval_data = retrieval_resp.json()\n\n    # Validate that retrieval response contains expected text or relevant info\n    assert \"answer\" in retrieval_data or \"results\" in retrieval_data, \"Retrieval response missing expected keys\"\n\n    # If 'answer' is available, check if it contains relevant info\n    if \"answer\" in retrieval_data:\n        answer = retrieval_data[\"answer\"]\n        assert \"LangGraph Builder\" in answer or \"AI agent workflows\" in answer, \"Answer doesn't contain expected content\"\n    elif \"results\" in retrieval_data and isinstance(retrieval_data[\"results\"], list):\n        text_found = any(\"LangGraph Builder\" in str(result) or \"AI agent workflows\" in str(result) for result in retrieval_data[\"results\"])\n        assert text_found, \"Results do not contain expected content\"\n    else:\n        assert False, \"Retrieval response format not recognized\"\n\n# Run the test\n\ntest_rag_dashboard_retrieval_testing()",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 46, in <module>\n  File \"<string>\", line 20, in test_rag_dashboard_retrieval_testing\nAssertionError: Upload response missing success status\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.716Z",
    "modified": "2025-12-05T11:05:44.235Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "a7302919-19c9-4146-be49-da762e46ff06",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC005-test_mcp_dashboard_server_registration",
    "description": "Verify that new MCP server connections can be registered via the MCP Dashboard and that the backend API correctly stores and manages these connections.",
    "code": "import requests\n\nBASE_URL = \"http://localhost:8001\"\nTIMEOUT = 30\nHEADERS = {\"Content-Type\": \"application/json\"}\n\ndef test_mcp_dashboard_server_registration():\n    new_server_data = {\n        \"name\": \"Test MCP Server\",\n        \"description\": \"Server registered by automated test\",\n        \"url\": \"http://test-mcp-server.local\",\n        \"api_key\": \"testapikey123\"\n    }\n\n    created_server_id = None\n\n    try:\n        # Register new MCP server connection\n        response = requests.post(\n            f\"{BASE_URL}/mcp/servers\",\n            json=new_server_data,\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert response.status_code == 201, f\"Expected 201 Created but got {response.status_code}\"\n        created_server = response.json()\n        assert \"id\" in created_server, \"Response JSON must contain 'id'\"\n        created_server_id = created_server[\"id\"]\n\n        # Verify that the server is listed and stored by the backend\n        list_response = requests.get(\n            f\"{BASE_URL}/mcp/servers\",\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert list_response.status_code == 200, f\"Expected 200 OK but got {list_response.status_code}\"\n        servers = list_response.json()\n        assert any(server[\"id\"] == created_server_id for server in servers), \"New server not found in server list\"\n\n        # Retrieve the specific server details and verify stored data\n        get_response = requests.get(\n            f\"{BASE_URL}/mcp/servers/{created_server_id}\",\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert get_response.status_code == 200, f\"Expected 200 OK but got {get_response.status_code}\"\n        server_details = get_response.json()\n        assert server_details[\"name\"] == new_server_data[\"name\"], \"Server name does not match\"\n        assert server_details[\"description\"] == new_server_data[\"description\"], \"Server description does not match\"\n        assert server_details[\"url\"] == new_server_data[\"url\"], \"Server URL does not match\"\n\n    finally:\n        # Cleanup: Delete the created MCP server to maintain test environment\n        if created_server_id:\n            delete_response = requests.delete(\n                f\"{BASE_URL}/mcp/servers/{created_server_id}\",\n                headers=HEADERS,\n                timeout=TIMEOUT,\n            )\n            assert delete_response.status_code == 204, f\"Expected 204 No Content on delete but got {delete_response.status_code}\"\n\ntest_mcp_dashboard_server_registration()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 62, in <module>\n  File \"<string>\", line 25, in test_mcp_dashboard_server_registration\nAssertionError: Expected 201 Created but got 422\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.722Z",
    "modified": "2025-12-05T11:05:44.237Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "b35010e3-837f-424a-88fb-2895626fce27",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC006-test_mcp_dashboard_tool_listing",
    "description": "Test that the MCP Dashboard can retrieve and display a list of available AI tools from registered MCP servers.",
    "code": "import requests\n\ndef test_mcp_dashboard_tool_listing():\n    base_url = \"http://localhost:8001\"\n    timeout = 30\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\",\n    }\n\n    # Step 1: Register a new MCP server to ensure at least one server exists\n    register_url = f\"{base_url}/mcp/servers\"\n    server_payload = {\n        \"name\": \"Test MCP Server\",\n        \"url\": \"http://test-mcp-server.local\",\n        \"description\": \"Temporary server for testing tool listing\",\n        \"api_key\": \"test-api-key\",\n        \"command\": \"start-server\"\n    }\n    server_id = None\n\n    try:\n        register_response = requests.post(register_url, json=server_payload, headers=headers, timeout=timeout)\n        assert register_response.status_code == 201, f\"Failed to register MCP server: {register_response.text}\"\n        server_data = register_response.json()\n        server_id = server_data.get(\"id\")\n        assert server_id is not None, \"No server id returned after registration\"\n\n        # Step 2: Retrieve the list of available AI tools from the registered MCP servers\n        tools_url = f\"{base_url}/mcp/servers/{server_id}/tools\"\n        tools_response = requests.get(tools_url, headers=headers, timeout=timeout)\n        assert tools_response.status_code == 200, f\"Failed to get tools list: {tools_response.text}\"\n        tools_list = tools_response.json()\n\n        # Validate that tools_list is a list and contains tool items with expected keys (e.g., name, id)\n        assert isinstance(tools_list, list), \"Tools response is not a list\"\n        if len(tools_list) > 0:\n            for tool in tools_list:\n                assert isinstance(tool, dict), \"Each tool should be a dictionary\"\n                assert \"id\" in tool, \"Tool item missing 'id'\"\n                assert \"name\" in tool, \"Tool item missing 'name'\"\n\n    finally:\n        # Cleanup: Delete the MCP server registered for the test if created\n        if server_id:\n            delete_url = f\"{base_url}/mcp/servers/{server_id}\"\n            try:\n                delete_response = requests.delete(delete_url, headers=headers, timeout=timeout)\n                assert delete_response.status_code in (200, 204), f\"Failed to delete MCP server: {delete_response.text}\"\n            except Exception:\n                pass\n\ntest_mcp_dashboard_tool_listing()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 53, in <module>\n  File \"<string>\", line 24, in test_mcp_dashboard_tool_listing\nAssertionError: Failed to register MCP server: {\"detail\":\"Failed to connect to MCP server Test MCP Server: [Errno 2] No such file or directory\"}\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.728Z",
    "modified": "2025-12-05T11:05:26.542Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "8ae03be0-f6e7-47ff-a832-289bae63597d",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC007-test_finetuning_dashboard_job_creation",
    "description": "Validate that users can create new fine-tuning jobs through the Fine-tuning Dashboard and that the backend API accepts and processes these job creation requests.",
    "code": "import requests\n\nBASE_URL = \"http://localhost:8001\"\nTIMEOUT = 30\nHEADERS = {\"Content-Type\": \"application/json\"}\n\ndef test_finetuning_dashboard_job_creation():\n    # Adjusted payload to match expected fine-tuning API schema\n    payload = {\n        \"model\": \"base-model\",\n        \"training_data\": [\n            {\"input\": \"Translate to French: Hello, world!\", \"output\": \"Bonjour le monde!\"},\n            {\"input\": \"Translate to Spanish: Goodbye!\", \"output\": \"¡Adiós!\"}\n        ],\n        \"hyperparameters\": {\n            \"epochs\": 3,\n            \"batch_size\": 8,\n            \"learning_rate\": 0.0001\n        },\n        \"name\": \"test-job-finetuning\"\n    }\n\n    job_id = None\n    try:\n        # Create a new fine-tuning job\n        response = requests.post(\n            f\"{BASE_URL}/finetune/jobs\",\n            headers=HEADERS,\n            json=payload,\n            timeout=TIMEOUT,\n        )\n        # Assert job creation success status code\n        assert response.status_code == 201, f\"Expected 201 Created, got {response.status_code}\"\n        data = response.json()\n        assert \"job_id\" in data, \"Response missing job_id\"\n        job_id = data[\"job_id\"]\n        assert data.get(\"status\") in {\"pending\", \"created\"}, \"Unexpected job status\"\n\n        # Retrieve job details to verify creation\n        get_response = requests.get(\n            f\"{BASE_URL}/finetune/jobs/{job_id}\",\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert get_response.status_code == 200, f\"Expected 200 OK retrieving job, got {get_response.status_code}\"\n        job_data = get_response.json()\n\n        # Validate returned job data matches what we sent\n        assert job_data.get(\"job_id\") == job_id\n        assert job_data.get(\"model\") == payload[\"model\"]\n        assert job_data.get(\"name\") == payload[\"name\"]\n        assert isinstance(job_data.get(\"training_data\"), list)\n        assert job_data.get(\"hyperparameters\") == payload[\"hyperparameters\"]\n        assert job_data.get(\"status\") in {\"pending\", \"created\", \"running\"}\n\n    finally:\n        # Cleanup: delete the created fine-tuning job if it was created\n        if job_id:\n            try:\n                del_response = requests.delete(\n                    f\"{BASE_URL}/finetune/jobs/{job_id}\",\n                    headers=HEADERS,\n                    timeout=TIMEOUT,\n                )\n                assert del_response.status_code in {200, 204, 202}, f\"Unexpected status deleting job: {del_response.status_code}\"\n            except Exception:\n                pass\n\n\ntest_finetuning_dashboard_job_creation()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 70, in <module>\n  File \"<string>\", line 33, in test_finetuning_dashboard_job_creation\nAssertionError: Expected 201 Created, got 422\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.735Z",
    "modified": "2025-12-05T11:06:02.954Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "12e187b4-d5f4-4456-8a27-cb880d8321ec",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC008-test_finetuning_dashboard_job_monitoring",
    "description": "Ensure that the Fine-tuning Dashboard provides real-time status updates for fine-tuning jobs and accurately reflects job progress and completion.",
    "code": "import requests\nimport time\n\nBASE_URL = \"http://localhost:8001\"\nHEADERS = {\"Content-Type\": \"application/json\"}\nTIMEOUT = 30\n\ndef test_finetuning_dashboard_job_monitoring():\n    create_job_url = f\"{BASE_URL}/finetuning/job\"\n    get_job_status_url_template = f\"{BASE_URL}/finetuning/job/{{job_id}}\"\n    \n    # Sample payload for creating a fine-tuning job (fields assumed based on typical fine-tuning jobs)\n    create_payload = {\n        \"model_name\": \"test-model\",\n        \"training_data\": [\n            {\"input\": \"Hello\", \"output\": \"World\"},\n            {\"input\": \"Test\", \"output\": \"Fine-tuning\"}\n        ],\n        \"hyperparameters\": {\n            \"epochs\": 1,\n            \"batch_size\": 1,\n            \"learning_rate\": 0.001\n        }\n    }\n    \n    job_id = None\n    try:\n        # Create a new fine-tuning job\n        create_response = requests.post(create_job_url, json=create_payload, headers=HEADERS, timeout=TIMEOUT)\n        assert create_response.status_code == 201, f\"Expected status code 201, got {create_response.status_code}\"\n        job_data = create_response.json()\n        job_id = job_data.get(\"id\")\n        assert job_id, \"Job ID not returned in create response\"\n        \n        # Poll the job status until it completes or fails, with a max timeout to avoid infinite wait\n        max_poll_time = 120  # seconds\n        poll_interval = 5    # seconds\n        time_spent = 0\n        \n        while time_spent < max_poll_time:\n            status_response = requests.get(get_job_status_url_template.format(job_id=job_id), headers=HEADERS, timeout=TIMEOUT)\n            assert status_response.status_code == 200, f\"Status check returned {status_response.status_code}\"\n            status_data = status_response.json()\n            \n            # Assuming the status_data contains fields 'status' and 'progress' as a percentage 0-100\n            status = status_data.get(\"status\")\n            progress = status_data.get(\"progress\")\n            assert status in {\"pending\", \"running\", \"completed\", \"failed\"}, f\"Invalid status: {status}\"\n            assert isinstance(progress, (int, float)) and 0 <= progress <= 100, f\"Invalid progress value: {progress}\"\n            \n            if status == \"completed\":\n                # Job finished successfully\n                break\n            if status == \"failed\":\n                assert False, \"Fine-tuning job failed during processing\"\n            \n            time.sleep(poll_interval)\n            time_spent += poll_interval\n        else:\n            assert False, \"Timeout waiting for fine-tuning job to complete\"\n            \n    finally:\n        if job_id:\n            # Delete the created job to clean up\n            delete_url = f\"{BASE_URL}/finetuning/job/{job_id}\"\n            try:\n                del_response = requests.delete(delete_url, headers=HEADERS, timeout=TIMEOUT)\n                assert del_response.status_code in {200, 204, 202}, f\"Failed to delete job {job_id}\"\n            except Exception:\n                pass\n\ntest_finetuning_dashboard_job_monitoring()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 72, in <module>\n  File \"<string>\", line 30, in test_finetuning_dashboard_job_monitoring\nAssertionError: Expected status code 201, got 404\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.740Z",
    "modified": "2025-12-05T11:05:52.754Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "3f017a01-7f15-46d3-9419-094b77104d8f",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC009-test_backend_api_graph_saving_and_loading",
    "description": "Test that graph configurations can be saved via the backend API and that saved configurations can be re-loaded correctly without data loss or corruption.",
    "code": "import requests\nimport uuid\n\nBASE_URL = \"http://localhost:8001\"\nTIMEOUT = 30\nHEADERS = {\"Content-Type\": \"application/json\"}\n\ndef test_backend_api_graph_saving_and_loading():\n    graph_config = {\n        \"id\": f\"test-graph-{uuid.uuid4()}\",\n        \"name\": \"Test Graph Configuration\",\n        \"description\": \"Graph config for TC009 validation\",\n        \"nodes\": [\n            {\n                \"id\": \"start-node\",\n                \"type\": \"StartNode\",\n                \"position\": {\"x\": 0, \"y\": 0},\n                \"data\": {},\n            },\n            {\n                \"id\": \"node-1\",\n                \"type\": \"LLMNode\",\n                \"position\": {\"x\": 100, \"y\": 150},\n                \"data\": {\"model_name\": \"gpt-4\", \"parameters\": {\"temperature\": 0.7}},\n            },\n            {\n                \"id\": \"node-2\",\n                \"type\": \"RAGNode\",\n                \"position\": {\"x\": 300, \"y\": 150},\n                \"data\": {\"retriever\": \"simple-retriever\", \"top_k\": 5},\n            }\n        ],\n        \"edges\": [\n            {\"id\": \"edge-start-1\", \"source\": \"start-node\", \"target\": \"node-1\", \"type\": \"default\"},\n            {\"id\": \"edge-1-2\", \"source\": \"node-1\", \"target\": \"node-2\", \"type\": \"default\"}\n        ],\n        \"metadata\": {\"created_by\": \"test_backend_api_graph_saving_and_loading\", \"version\": 1}\n    }\n\n    try:\n        # Save the graph configuration\n        save_response = requests.post(\n            f\"{BASE_URL}/graphs/\",\n            headers=HEADERS,\n            json=graph_config,\n            timeout=TIMEOUT,\n        )\n        assert save_response.status_code == 201, f\"Failed to save graph config: {save_response.text}\"\n        saved_graph = save_response.json()\n        assert saved_graph.get(\"id\") == graph_config[\"id\"], \"Saved graph ID mismatch\"\n\n        # Load the saved graph configuration\n        load_response = requests.get(\n            f\"{BASE_URL}/graphs/{graph_config['id']}\",\n            headers=HEADERS,\n            timeout=TIMEOUT,\n        )\n        assert load_response.status_code == 200, f\"Failed to load graph config: {load_response.text}\"\n        loaded_graph = load_response.json()\n\n        # Validate loaded graph equals saved one (deep check)\n        assert loaded_graph.get(\"id\") == graph_config[\"id\"], \"Loaded graph ID mismatch\"\n        assert loaded_graph.get(\"name\") == graph_config[\"name\"], \"Graph name mismatch after loading\"\n        assert loaded_graph.get(\"description\") == graph_config[\"description\"], \"Graph description mismatch after loading\"\n        assert loaded_graph.get(\"metadata\") == graph_config[\"metadata\"], \"Graph metadata mismatch after loading\"\n\n        # Validate nodes and edges\n        loaded_nodes = loaded_graph.get(\"nodes\")\n        loaded_edges = loaded_graph.get(\"edges\")\n        assert isinstance(loaded_nodes, list), \"Loaded nodes is not a list\"\n        assert isinstance(loaded_edges, list), \"Loaded edges is not a list\"\n        assert len(loaded_nodes) == len(graph_config[\"nodes\"]), \"Number of nodes mismatch after loading\"\n        assert len(loaded_edges) == len(graph_config[\"edges\"]), \"Number of edges mismatch after loading\"\n\n        # Check details of nodes\n        for original_node in graph_config[\"nodes\"]:\n            matched_node = next((n for n in loaded_nodes if n[\"id\"] == original_node[\"id\"]), None)\n            assert matched_node is not None, f\"Node {original_node['id']} missing after load\"\n            assert matched_node[\"type\"] == original_node[\"type\"], f\"Node type mismatch for {original_node['id']}\"\n            assert matched_node[\"position\"] == original_node[\"position\"], f\"Node position mismatch for {original_node['id']}\"\n            assert matched_node[\"data\"] == original_node[\"data\"], f\"Node data mismatch for {original_node['id']}\"\n\n        # Check details of edges\n        for original_edge in graph_config[\"edges\"]:\n            matched_edge = next((e for e in loaded_edges if e[\"id\"] == original_edge[\"id\"]), None)\n            assert matched_edge is not None, f\"Edge {original_edge['id']} missing after load\"\n            assert matched_edge[\"source\"] == original_edge[\"source\"], f\"Edge source mismatch for {original_edge['id']}\"\n            assert matched_edge[\"target\"] == original_edge[\"target\"], f\"Edge target mismatch for {original_edge['id']}\"\n            assert matched_edge[\"type\"] == original_edge[\"type\"], f\"Edge type mismatch for {original_edge['id']}\"\n\n    finally:\n        # Cleanup: delete the graph configuration if created\n        try:\n            requests.delete(\n                f\"{BASE_URL}/graphs/{graph_config['id']}\",\n                headers=HEADERS,\n                timeout=TIMEOUT,\n            )\n        except Exception:\n            pass\n\ntest_backend_api_graph_saving_and_loading()\n",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 102, in <module>\n  File \"<string>\", line 48, in test_backend_api_graph_saving_and_loading\nAssertionError: Failed to save graph config: {\"detail\":\"Unknown node type: StartNode\"}\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.746Z",
    "modified": "2025-12-05T11:06:28.524Z"
  },
  {
    "projectId": "97107430-a56d-4e5d-992d-eaa6c427bb50",
    "testId": "b04822eb-ea06-4992-b013-c95613333bb6",
    "userId": "8428f4e8-d0c1-70fd-dc0b-a95982197a9e",
    "title": "TC010-test_backend_api_error_handling_for_invalid_inputs",
    "description": "Verify that the backend API handles invalid user inputs gracefully by returning appropriate error messages and preventing faulty states in the system.",
    "code": "import requests\n\nBASE_URL = \"http://localhost:8001\"\nHEADERS = {\"Content-Type\": \"application/json\"}\nTIMEOUT = 30\n\ndef test_backend_api_error_handling_for_invalid_inputs():\n    \"\"\"\n    Verify that the backend API handles invalid user inputs gracefully by returning appropriate error messages\n    and preventing faulty states in the system.\n    The test tries to send invalid inputs to various endpoints to check error handling.\n    \"\"\"\n\n    # List of endpoint tests with method, url path, and invalid payloads or parameters\n    tests = [\n        # Graph save with invalid payload (missing required fields)\n        {\n            \"method\": \"POST\",\n            \"url\": \"/graph/save\",\n            \"json\": {\"invalidField\": 123},\n            \"expected_status\": 422\n        },\n        # Execute graph node with invalid node id type\n        {\n            \"method\": \"POST\",\n            \"url\": \"/graph/execute\",\n            \"json\": {\"nodeId\": \"not_an_int\"},\n            \"expected_status\": 422\n        },\n        # RAG document upload with invalid file data (missing required file content)\n        {\n            \"method\": \"POST\",\n            \"url\": \"/rag/upload\",\n            \"json\": {\"filename\": \"\", \"content\": \"\"},\n            \"expected_status\": 422\n        },\n        # MCP server registration with invalid connection string format\n        {\n            \"method\": \"POST\",\n            \"url\": \"/mcp/register\",\n            \"json\": {\"serverName\": \"\", \"connectionString\": \"invalid-url\"},\n            \"expected_status\": 422\n        },\n        # Fine-tuning job creation with missing parameters\n        {\n            \"method\": \"POST\",\n            \"url\": \"/finetune/create\",\n            \"json\": {\"model\": \"\", \"trainingData\": None},\n            \"expected_status\": 422\n        },\n        # Invalid HTTP method on a valid endpoint\n        {\n            \"method\": \"PUT\",\n            \"url\": \"/graph/save\",\n            \"json\": {},\n            \"expected_status\": 405\n        },\n        # Invalid route\n        {\n            \"method\": \"GET\",\n            \"url\": \"/nonexistent/endpoint\",\n            \"json\": None,\n            \"expected_status\": 404\n        }\n    ]\n\n    for test_case in tests:\n        method = test_case[\"method\"].lower()\n        url = BASE_URL + test_case[\"url\"]\n        json_data = test_case[\"json\"]\n        expected_status = test_case[\"expected_status\"]\n\n        try:\n            if method == \"get\":\n                response = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n            elif method == \"post\":\n                response = requests.post(url, headers=HEADERS, json=json_data, timeout=TIMEOUT)\n            elif method == \"put\":\n                response = requests.put(url, headers=HEADERS, json=json_data, timeout=TIMEOUT)\n            elif method == \"delete\":\n                response = requests.delete(url, headers=HEADERS, json=json_data, timeout=TIMEOUT)\n            else:\n                # Unsupported method for this test - skip\n                continue\n        except requests.RequestException as e:\n            assert False, f\"Request to {url} with method {method} failed unexpectedly: {str(e)}\"\n\n        assert response.status_code == expected_status, (\n            f\"Expected status {expected_status} but got {response.status_code} \"\n            f\"for {method.upper()} {url} with payload {json_data}. \"\n            f\"Response body: {response.text}\"\n        )\n\n        # For error responses, check error message presence\n        if expected_status >= 400:\n            try:\n                data = response.json()\n                assert (\"detail\" in data) or (\"error\" in data) or (\"message\" in data), \\\n                    f\"Error response missing expected keys in JSON body for {url}: {data}\"\n            except Exception:\n                # If response not JSON, ensure something meaningful is returned instead of empty\n                assert len(response.text) > 0, f\"Empty error response body for {url}\"\n\ntest_backend_api_error_handling_for_invalid_inputs()",
    "testStatus": "FAILED",
    "testError": "Traceback (most recent call last):\n  File \"/var/task/handler.py\", line 258, in run_with_retry\n    exec(code, exec_env)\n  File \"<string>\", line 104, in <module>\n  File \"<string>\", line 88, in test_backend_api_error_handling_for_invalid_inputs\nAssertionError: Expected status 422 but got 404 for POST http://localhost:8001/graph/save with payload {'invalidField': 123}. Response body: {\"detail\":\"Not Found\"}\n",
    "testType": "BACKEND",
    "createFrom": "mcp",
    "created": "2025-12-05T11:04:34.751Z",
    "modified": "2025-12-05T11:05:40.412Z"
  }
]
